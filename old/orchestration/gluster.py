# pylint: disable=duplicate-code
"""GlusterFS distributed storage orchestration."""
from __future__ import annotations
import time
from dataclasses import dataclass
from typing import Optional, Sequence, Tuple
from cli import Apt, CommandWrapper, Gluster, SystemCtl
from libs import common
from libs.config import LabConfig
from libs.logger import get_logger
from services.lxc import LXCService
from services.pct import PCTService
logger = get_logger(__name__)
ssh_exec = common.ssh_exec
destroy_container = common.destroy_container
wait_for_container = common.wait_for_container
container_exists = common.container_exists
@dataclass(frozen=True)

class NodeInfo:
    """Minimal representation of a container needed for orchestration steps."""
    container_id: int
    hostname: str
    ip_address: str
    @classmethod

    def from_container(cls, container_cfg):
        """Build node info from container configuration."""
        return cls(container_id=container_cfg.id, hostname=container_cfg.hostname, ip_address=container_cfg.ip_address)

def setup_glusterfs(cfg: LabConfig):
    """Setup GlusterFS distributed storage"""
    logger.info("\n[5/7] Setting up GlusterFS distributed storage...")
    if not cfg.glusterfs:
        logger.info("GlusterFS configuration not found, skipping...")
        return True
    gluster_cfg = cfg.glusterfs
    manager, workers = _collect_gluster_nodes(cfg)
    if not manager or not workers:
        return False
    all_nodes = [manager] + workers
    apt_cache_ip, apt_cache_port = _get_apt_cache_proxy(cfg)
    proxy_settings = (apt_cache_ip, apt_cache_port)
    logger.info("Installing GlusterFS server on all nodes...")
    failure_detected = False
    ordered_steps = [
        lambda: _fix_apt_sources(all_nodes, cfg),
        lambda: _install_gluster_packages(all_nodes, proxy_settings, cfg),
        lambda: _delay(cfg.waits.glusterfs_setup),
        lambda: _create_bricks(all_nodes, gluster_cfg.brick_path, cfg),
    ]
    for step in ordered_steps:
        if not step():
            failure_detected = True
            break
    gluster_cmd = None
    if not failure_detected:
        gluster_cmd = _resolve_gluster_cmd(manager, cfg)
        if not gluster_cmd:
            failure_detected = True
    if not failure_detected and not _peer_workers(manager, workers, gluster_cmd, cfg):
        failure_detected = True
    if not failure_detected:
        peers_ready = _wait_for_peers(manager, workers, gluster_cmd, cfg)
        if not peers_ready:
            logger.warning("Not all peers may be fully connected, continuing anyway...")
    if not failure_detected and not _ensure_volume(manager, workers, gluster_cmd, gluster_cfg, cfg):
        failure_detected = True
    if not failure_detected and not _mount_gluster_volume(manager, workers, gluster_cfg, cfg):
        failure_detected = True
    # Mount GlusterFS on Swarm and K3s nodes as clients
    if not failure_detected:
        client_mount_result = _mount_gluster_on_clients(manager, gluster_cfg, cfg)
        if not client_mount_result:
            failure_detected = True
    if failure_detected:
        return False
    _log_gluster_summary(gluster_cfg)
    return True

def _collect_gluster_nodes(cfg: LabConfig) -> Tuple[Optional[NodeInfo], Sequence[NodeInfo]]:
    """Collect GlusterFS nodes from dedicated cluster_nodes"""
    if not cfg.glusterfs:
        return None, []
    
    # Check if dedicated cluster nodes are configured
    if not cfg.glusterfs.cluster_nodes:
        logger.error("GlusterFS cluster_nodes configuration not found")
        return None, []
    
    cluster_node_ids = [node["id"] for node in cfg.glusterfs.cluster_nodes]
    gluster_nodes = [
        NodeInfo.from_container(container)
        for container in cfg.containers
        if container.id in cluster_node_ids
    ]
    if len(gluster_nodes) < 2:
        logger.error("Need at least 2 GlusterFS cluster nodes, found %d", len(gluster_nodes))
        return None, []
    # First node is manager, rest are workers
    return gluster_nodes[0], gluster_nodes[1:]

def _get_apt_cache_proxy(cfg: LabConfig):
    """Return apt-cache proxy settings if available."""
    apt_cache = next((c for c in cfg.containers if c.name == cfg.apt_cache_ct), None)
    if not apt_cache:
        return None, None
    return apt_cache.ip_address, cfg.apt_cache_port

def _delay(seconds):
    """Sleep helper that always returns True for step sequencing."""
    time.sleep(seconds)
    return True

def _fix_apt_sources(nodes, cfg):
    """Ensure all nodes use the expected Ubuntu sources."""
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        for node in nodes:
            _fix_apt_sources_for_node(node, cfg, pct_service)
        return True
    finally:
        lxc_service.disconnect()

def _fix_apt_sources_for_node(node, cfg, pct_service):
    """Fix apt sources for a single node."""
    sources_cmd = " ".join(
        [
            ("sed -i 's/oracular/plucky/g' /etc/apt/sources.list " "2>/dev/null || true;"),
            ("if ! grep -q '^deb.*plucky.*main' /etc/apt/sources.list; then"),
            (
                "echo 'deb http://archive.ubuntu.com/ubuntu plucky main "
                "universe multiverse' > /etc/apt/sources.list;"
            ),
            (
                "echo 'deb http://archive.ubuntu.com/ubuntu plucky-updates main "
                "universe multiverse' >> /etc/apt/sources.list;"
            ),
            (
                "echo 'deb http://archive.ubuntu.com/ubuntu plucky-security main "
                "universe multiverse' >> /etc/apt/sources.list;"
            ),
            "fi 2>&1",
        ]
    )
    # Handle both NodeInfo (has container_id) and ContainerConfig (has id)
    node_id = str(node.container_id if hasattr(node, 'container_id') else node.id)
    node_hostname = node.hostname if hasattr(node, 'hostname') else getattr(node, 'name', 'unknown')
    
    sources_result, _ = pct_service.execute(node_id, sources_cmd)
    if sources_result and "error" in sources_result.lower():
        logger.warning("Apt sources fix had issues on %s: %s", node_hostname, sources_result[-200:])

def _install_gluster_packages(nodes, proxy_settings, cfg):
    """Install GlusterFS packages and ensure glusterd is running on each node."""
    for node in nodes:
        logger.info("Installing on %s...", node.hostname)
        if not _configure_gluster_node(node, proxy_settings, cfg):
            return False
    return True

def _configure_gluster_node(node, proxy_settings, cfg, max_retries=2):
    """Configure GlusterFS packages on a single node."""
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        for attempt in range(1, max_retries + 1):
            _configure_proxy(node.container_id, attempt == 1, proxy_settings, cfg, pct_service)
            update_cmd = Apt.update_cmd()
            update_output, _ = pct_service.execute(str(node.container_id), update_cmd, timeout=600)
            update_result = CommandWrapper.parse_result(update_output)
            if _should_retry_update(update_result) and attempt < max_retries:
                logger.warning("apt update failed, will retry without proxy...")
                continue
            install_cmd = Apt.install_cmd(["glusterfs-server", "glusterfs-client"])
            install_output, _ = pct_service.execute(str(node.container_id), install_cmd, timeout=300)
            install_result = CommandWrapper.parse_result(install_output)
            verify_cmd = Gluster().gluster_cmd("gluster").is_installed_check()
            verify_output, _ = pct_service.execute(str(node.container_id), verify_cmd, timeout=10)
            if Gluster.parse_is_installed(verify_output):
                logger.info("GlusterFS installed successfully on %s", node.hostname)
                return _ensure_glusterd_running(node, cfg, pct_service)
            logger.warning(
                "Installation attempt %s failed on %s: %s - %s",
                attempt,
                node.hostname,
                install_result.error_type.value if install_result.error_type else "unknown",
                install_result.error_message,
            )
            if attempt < max_retries:
                logger.info("Retrying without proxy...")
                time.sleep(2)
        logger.error("Failed to install GlusterFS on %s after %s attempts", node.hostname, max_retries)
        return False
    finally:
        lxc_service.disconnect()

def _configure_proxy(container_id, use_proxy, proxy_settings, cfg, pct_service):
    """Enable or disable apt proxy on a node."""
    apt_cache_ip, apt_cache_port = proxy_settings
    if use_proxy and apt_cache_ip and apt_cache_port:
        proxy_cmd = (
            "echo 'Acquire::http::Proxy "
            f'"http://{apt_cache_ip}:{apt_cache_port}";\' '
            "> /etc/apt/apt.conf.d/01proxy || true 2>&1"
        )
        proxy_result, _ = pct_service.execute(str(container_id), proxy_cmd, timeout=10)
        if proxy_result and "error" in proxy_result.lower():
            logger.warning("Proxy configuration had issues: %s", proxy_result[-200:])
    else:
        rm_proxy_result, _ = pct_service.execute(str(container_id), "rm -f /etc/apt/apt.conf.d/01proxy 2>&1", timeout=10)
        if rm_proxy_result and "error" in rm_proxy_result.lower():
            logger.warning("Proxy removal had issues: %s", rm_proxy_result[-200:])

def _should_retry_update(update_result):
    """Determine if apt update should be retried without proxy."""
    return bool(
        update_result.has_error
        or (
            update_result.output
            and ("Failed to fetch" in update_result.output or "Unable to connect" in update_result.output)
        )
    )

def _ensure_glusterd_running(node, cfg, pct_service):
    """Enable, start, and verify glusterd on a node."""
    logger.info("Starting glusterd service on %s...", node.hostname)
    glusterd_start_cmd = SystemCtl().service("glusterd").enable_and_start()
    glusterd_start_output, _ = pct_service.execute(str(node.container_id), glusterd_start_cmd, timeout=30)
    glusterd_start_result = CommandWrapper.parse_result(glusterd_start_output)
    if glusterd_start_result.has_error:
        logger.error(
            "Failed to start glusterd on %s: %s - %s",
            node.hostname,
            glusterd_start_result.error_type.value,
            glusterd_start_result.error_message,
        )
        return False
    time.sleep(3)
    is_active_cmd = SystemCtl().service("glusterd").is_active()
    glusterd_check_output, _ = pct_service.execute(str(node.container_id), is_active_cmd, timeout=10)
    if SystemCtl.parse_is_active(glusterd_check_output):
        logger.info("%s: GlusterFS installed and glusterd running", node.hostname)
        return True
    logger.error("%s: GlusterFS installed but glusterd is not running: %s", node.hostname, glusterd_check_output)
    return False

def _create_bricks(nodes, brick_path, cfg):
    """Create brick directories on all nodes."""
    logger.info("Creating brick directories on all nodes...")
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        for node in nodes:
            logger.info("Creating brick on %s...", node.hostname)
            brick_result, _ = pct_service.execute(str(node.container_id), f"mkdir -p {brick_path} && chmod 755 {brick_path} 2>&1")
            if brick_result and "error" in brick_result.lower():
                logger.error("Failed to create brick directory on %s: %s", node.hostname, brick_result[-300:])
                return False
        return True
    finally:
        lxc_service.disconnect()

def _resolve_gluster_cmd(manager: NodeInfo, cfg):
    """Find the gluster executable inside the manager container."""
    lxc_service = LXCService(cfg.proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return None
    try:
        pct_service = PCTService(lxc_service)
        find_gluster_cmd = Gluster().find_gluster()
        gluster_path, _ = pct_service.execute(str(manager.container_id), find_gluster_cmd, timeout=10)
        if not gluster_path:
            logger.error("Unable to locate gluster binary")
            return None
        if gluster_path and gluster_path.strip():
            # Take only the first line (first path found)
            first_line = gluster_path.strip().split("\n")[0].strip()
            return first_line if first_line else "gluster"
        return "gluster"
    finally:
        lxc_service.disconnect()

def _peer_workers(manager, workers, gluster_cmd, cfg):
    """Peer all worker nodes to the manager."""
    logger.info("Peering worker nodes together...")
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        for worker in workers:
            logger.info("Adding %s (%s) to cluster...", worker.hostname, worker.ip_address)
            probe_cmd = (
                f"{Gluster().gluster_cmd(gluster_cmd).peer_probe(worker.hostname)} || "
                f"{Gluster().gluster_cmd(gluster_cmd).peer_probe(worker.ip_address)}"
            )
            probe_output, _ = pct_service.execute(str(manager.container_id), probe_cmd)
            probe_result = CommandWrapper.parse_result(probe_output)
            if (
                probe_result.has_error
                and "already" not in (probe_output or "").lower()
                and "already in peer list" not in (probe_output or "").lower()
            ):
                logger.warning(
                    "Peer probe had issues for %s: %s - %s",
                    worker.hostname,
                    probe_result.error_type.value,
                    probe_result.error_message,
                )
        time.sleep(10)
        return True
    finally:
        lxc_service.disconnect()

def _wait_for_peers(manager, workers, gluster_cmd, cfg):
    """Wait until all peers report as connected."""
    logger.info("Verifying peer status...")
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        max_peer_attempts = 10
        for attempt in range(1, max_peer_attempts + 1):
            peer_status_cmd = Gluster().gluster_cmd(gluster_cmd).peer_status()
            peer_status, _ = pct_service.execute(str(manager.container_id), peer_status_cmd)
            if not peer_status:
                logger.warning("No peer status output received")
                if attempt < max_peer_attempts:
                    logger.info("Waiting for peers to connect... (%s/%s)", attempt, max_peer_attempts)
                    time.sleep(3)
                    continue
                return False
            logger.info(peer_status)
            connected_count = peer_status.count("Peer in Cluster (Connected)")
            if connected_count >= len(workers):
                logger.info("All %s worker peers connected", connected_count)
                return True
            if attempt < max_peer_attempts:
                logger.info("Waiting for peers to connect... (%s/%s)", attempt, max_peer_attempts)
                time.sleep(3)
        return False
    finally:
        lxc_service.disconnect()

def _ensure_volume(  # pylint: disable=too-many-locals
    manager,
    workers,
    gluster_cmd,
    gluster_cfg,
    cfg,
):
    """Create the Gluster volume if needed and ensure it is running."""
    proxmox_host = cfg.proxmox_host
    volume_name = gluster_cfg.volume_name
    brick_path = gluster_cfg.brick_path
    replica_count = gluster_cfg.replica_count
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        logger.info("Creating GlusterFS volume '%s'...", volume_name)
        volume_exists_cmd = Gluster().gluster_cmd(gluster_cmd).volume_exists_check(volume_name)
        volume_exists_output, _ = pct_service.execute(str(manager.container_id), volume_exists_cmd)
        if Gluster.parse_volume_exists(volume_exists_output):
            logger.info("Volume '%s' already exists", volume_name)
            return True
        # Include all nodes (manager + workers) in brick list for replica setup
        all_nodes = [manager] + list(workers)
        brick_list = [f"{node.ip_address}:{brick_path}" for node in all_nodes]
        create_cmd = Gluster().gluster_cmd(gluster_cmd).force().volume_create(volume_name, replica_count, brick_list)
        create_output, _ = pct_service.execute(str(manager.container_id), create_cmd)
        create_result = CommandWrapper.parse_result(create_output)
        logger.info("%s", create_output)
        if not (
            create_result.success
            or "created" in (create_output or "").lower()
            or "success" in (create_output or "").lower()
        ):
            logger.error("Volume creation failed: %s - %s", create_result.error_type.value, create_result.error_message)
            return False
        logger.info("Starting volume '%s'...", volume_name)
        start_cmd = Gluster().gluster_cmd(gluster_cmd).volume_start(volume_name)
        start_output, _ = pct_service.execute(str(manager.container_id), start_cmd)
        logger.info("%s", start_output)
        logger.info("Verifying volume status...")
        vol_status_cmd = Gluster().gluster_cmd(gluster_cmd).volume_status(volume_name)
        vol_status, _ = pct_service.execute(str(manager.container_id), vol_status_cmd)
        if vol_status:
            logger.info(vol_status)
        return True
    finally:
        lxc_service.disconnect()

def _mount_gluster_volume(manager, workers, gluster_cfg, cfg,
):
    """Mount Gluster volume on manager and worker nodes."""
    nodes = [manager] + workers
    volume_name = gluster_cfg.volume_name
    mount_point = gluster_cfg.mount_point
    proxmox_host = cfg.proxmox_host
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        return False
    try:
        pct_service = PCTService(lxc_service)
        logger.info("Mounting GlusterFS volume on all nodes...")
        for node in nodes:
            logger.info("Mounting on %s...", node.hostname)
            mkdir_result, _ = pct_service.execute(str(node.container_id), f"mkdir -p {mount_point} 2>&1")
            if mkdir_result and "error" in mkdir_result.lower():
                logger.error("Failed to create mount point on %s: %s", node.hostname, mkdir_result[-300:])
                return False
            fstab_entry = f"{manager.hostname}:/{volume_name} {mount_point} " "glusterfs defaults,_netdev 0 0"
            fstab_cmd = " ".join([f"grep -q '{mount_point}' /etc/fstab", f"|| echo '{fstab_entry}' >> /etc/fstab 2>&1"])
            fstab_result, _ = pct_service.execute(str(node.container_id), fstab_cmd)
            if fstab_result and "error" in fstab_result.lower():
                logger.warning("fstab update had issues on %s: %s", node.hostname, fstab_result[-200:])
            # Use mount.glusterfs directly with full path instead of mount -t glusterfs
            mount_cmd = " ".join(
                [
                    f"/usr/sbin/mount.glusterfs {manager.hostname}:/{volume_name} {mount_point} 2>&1",
                    "||",
                    f"/usr/sbin/mount.glusterfs {manager.ip_address}:/{volume_name} {mount_point} 2>&1",
                ]
            )
            mount_result, _ = pct_service.execute(str(node.container_id), mount_cmd)
            if mount_result and "error" in mount_result.lower() and "already mounted" not in mount_result.lower():
                logger.error("Failed to mount GlusterFS on %s: %s", node.hostname, mount_result[-300:])
                return False
            if not _verify_mount(node, mount_point, cfg, pct_service):
                return False
        return True
    finally:
        lxc_service.disconnect()

def _verify_mount(node, mount_point, cfg, pct_service):
    """Verify Gluster mount status on a node."""
    mount_verify_cmd = " ".join(
        [
            f"mount | grep -q '{mount_point}'",
            "&& mount | grep '{mount_point}' | grep -q gluster",
            "&& echo mounted || echo not_mounted",
        ]
    )
    mount_verify, _ = pct_service.execute(str(node.container_id), mount_verify_cmd)
    if mount_verify and "mounted" in mount_verify and "not_mounted" not in mount_verify:
        logger.info("%s: Volume mounted successfully", node.hostname)
        return True
    mount_info_cmd = f"mount | grep {mount_point} 2>/dev/null || echo 'NOT_MOUNTED'"
    mount_info, _ = pct_service.execute(str(node.container_id), mount_info_cmd)
    if mount_info and ("NOT_MOUNTED" in mount_info or not mount_info.strip()):
        logger.error("%s: Mount failed - volume not mounted", node.hostname)
        return False
    logger.warning("%s: Mount status unclear - %s", node.hostname, mount_info[:80] if mount_info else "No output")
    return True

def _mount_gluster_on_clients(manager, gluster_cfg, cfg):
    """Mount GlusterFS volume on Swarm and K3s nodes as clients."""
    proxmox_host = cfg.proxmox_host
    volume_name = gluster_cfg.volume_name
    mount_point = gluster_cfg.mount_point
    
    # Collect K3s nodes
    client_nodes = []
    if cfg.kubernetes:
        if cfg.kubernetes.control:
            client_nodes.extend([c for c in cfg.containers if c.id in cfg.kubernetes.control])
        if cfg.kubernetes.workers:
            client_nodes.extend([c for c in cfg.containers if c.id in cfg.kubernetes.workers])
    
    # Remove duplicates
    client_nodes = list({c.id: c for c in client_nodes}.values())
    
    if not client_nodes:
        logger.info("No K3s nodes found for GlusterFS client mounting")
        return True
    
    lxc_service = LXCService(proxmox_host, cfg.ssh)
    if not lxc_service.connect():
        logger.error("Failed to connect to Proxmox host for client mounting")
        return False
    
    try:
        pct_service = PCTService(lxc_service)
        logger.info("Mounting GlusterFS volume on %d client nodes...", len(client_nodes))
        
        # Install glusterfs-client on client nodes
        installation_failed = []
        for node in client_nodes:
            logger.info("Installing glusterfs-client on %s...", node.hostname)
            # Check if mount.glusterfs exists at expected location (more reliable than package check)
            verify_cmd = "test -x /usr/sbin/mount.glusterfs && echo installed || echo not_installed"
            verify_output, _ = pct_service.execute(str(node.id), verify_cmd, timeout=10)
            # Check for exact match - "installed" is substring of "not_installed" so need exact check
            if verify_output and verify_output.strip() == "installed":
                logger.info("glusterfs-client already installed on %s", node.hostname)
                continue
            
            # Fix apt sources if needed (same as gluster nodes)
            _fix_apt_sources_for_node(node, cfg, pct_service)
            
            # Update apt
            update_cmd = Apt.update_cmd()
            update_output, update_exit = pct_service.execute(str(node.id), update_cmd, timeout=600)
            if update_exit is not None and update_exit != 0:
                logger.error("Failed to update apt on %s: %s", node.hostname, update_output[-200:] if update_output else "No output")
                installation_failed.append(node.hostname)
                continue
            
            # Install glusterfs-client
            install_cmd = Apt.install_cmd(["glusterfs-client"])
            install_output, exit_code = pct_service.execute(str(node.id), install_cmd, timeout=300)
            if exit_code is not None and exit_code != 0:
                logger.error("Failed to install glusterfs-client on %s: %s", node.hostname, install_output[-300:] if install_output else "No output")
                installation_failed.append(node.hostname)
                continue
            
            # Verify installation - check for mount.glusterfs at expected location
            verify_cmd = "test -x /usr/sbin/mount.glusterfs && echo installed || echo not_installed"
            verify_output, _ = pct_service.execute(str(node.id), verify_cmd, timeout=10)
            # Check for exact match - "installed" is substring of "not_installed" so need exact check
            if verify_output and verify_output.strip() != "installed":
                logger.error("glusterfs-client installation verification failed on %s - /usr/sbin/mount.glusterfs not found", node.hostname)
                installation_failed.append(node.hostname)
        
        if installation_failed:
            logger.error("Failed to install glusterfs-client on %d node(s): %s", len(installation_failed), ", ".join(installation_failed))
            return False
        
        # Mount on client nodes
        mount_failed = []
        for node in client_nodes:
            logger.info("Mounting GlusterFS on %s...", node.hostname)
            mkdir_cmd = f"mkdir -p {mount_point}"
            mkdir_result, mkdir_exit = pct_service.execute(str(node.id), mkdir_cmd, timeout=10)
            if mkdir_exit is not None and mkdir_exit != 0:
                logger.error("Failed to create mount point on %s: %s", node.hostname, mkdir_result[-200:] if mkdir_result else "No output")
                mount_failed.append(node.hostname)
                continue
            
            fstab_entry = f"{manager.hostname}:/{volume_name} {mount_point} glusterfs defaults,_netdev 0 0"
            fstab_cmd = f"grep -q '{mount_point}' /etc/fstab || echo '{fstab_entry}' >> /etc/fstab"
            fstab_result, _ = pct_service.execute(str(node.id), fstab_cmd, timeout=10)
            # Reload systemd after fstab modification
            reload_cmd = "systemctl daemon-reload"
            pct_service.execute(str(node.id), reload_cmd, timeout=10)
            
            # Use mount.glusterfs directly with full path instead of mount -t glusterfs
            mount_cmd = (
                f"/usr/sbin/mount.glusterfs {manager.hostname}:/{volume_name} {mount_point} 2>&1 || "
                f"/usr/sbin/mount.glusterfs {manager.ip_address}:/{volume_name} {mount_point} 2>&1"
            )
            mount_result, mount_exit = pct_service.execute(str(node.id), mount_cmd, timeout=30)
            if mount_exit is not None and mount_exit != 0:
                if mount_result and "already mounted" not in mount_result.lower():
                    logger.error("Failed to mount GlusterFS on %s: %s", node.hostname, mount_result[-300:] if mount_result else "No output")
                    mount_failed.append(node.hostname)
                else:
                    logger.info("GlusterFS already mounted on %s", node.hostname)
            else:
                # Verify mount
                verify_mount_cmd = f"mount | grep -q '{mount_point}' && mount | grep '{mount_point}' | grep -q gluster && echo mounted || echo not_mounted"
                verify_mount_output, _ = pct_service.execute(str(node.id), verify_mount_cmd, timeout=10)
                if verify_mount_output and "mounted" in verify_mount_output:
                    logger.info("GlusterFS mounted successfully on %s", node.hostname)
                else:
                    logger.error("Mount verification failed on %s", node.hostname)
                    mount_failed.append(node.hostname)
        
        if mount_failed:
            logger.error("Failed to mount GlusterFS on %d node(s): %s", len(mount_failed), ", ".join(mount_failed))
            return False
        
        return True
    finally:
        lxc_service.disconnect()

def _log_gluster_summary(gluster_cfg):
    """Print a concise summary of GlusterFS deployment."""
    logger.info("GlusterFS distributed storage setup complete")
    logger.info("  Volume: %s", gluster_cfg.volume_name)
    logger.info("  Mount point: %s on all nodes", gluster_cfg.mount_point)
    logger.info("  Replication: %sx", gluster_cfg.replica_count)
